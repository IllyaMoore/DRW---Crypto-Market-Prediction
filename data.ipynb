{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df9ee6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp311-cp311-win_amd64.whl (11.3 MB)\n",
      "                                              0.0/11.3 MB ? eta -:--:--\n",
      "                                              0.0/11.3 MB ? eta -:--:--\n",
      "                                              0.1/11.3 MB 1.7 MB/s eta 0:00:07\n",
      "                                              0.2/11.3 MB 1.5 MB/s eta 0:00:08\n",
      "                                              0.3/11.3 MB 1.7 MB/s eta 0:00:07\n",
      "     -                                        0.4/11.3 MB 1.6 MB/s eta 0:00:07\n",
      "     -                                        0.5/11.3 MB 1.8 MB/s eta 0:00:07\n",
      "     --                                       0.6/11.3 MB 2.0 MB/s eta 0:00:06\n",
      "     --                                       0.8/11.3 MB 2.1 MB/s eta 0:00:06\n",
      "     ---                                      0.9/11.3 MB 2.2 MB/s eta 0:00:05\n",
      "     ---                                      1.0/11.3 MB 2.3 MB/s eta 0:00:05\n",
      "     ----                                     1.2/11.3 MB 2.5 MB/s eta 0:00:05\n",
      "     -----                                    1.5/11.3 MB 2.7 MB/s eta 0:00:04\n",
      "     -----                                    1.7/11.3 MB 2.9 MB/s eta 0:00:04\n",
      "     ------                                   1.9/11.3 MB 3.0 MB/s eta 0:00:04\n",
      "     -------                                  2.1/11.3 MB 3.1 MB/s eta 0:00:03\n",
      "     --------                                 2.3/11.3 MB 3.2 MB/s eta 0:00:03\n",
      "     --------                                 2.5/11.3 MB 3.3 MB/s eta 0:00:03\n",
      "     ---------                                2.7/11.3 MB 3.3 MB/s eta 0:00:03\n",
      "     ----------                               3.0/11.3 MB 3.4 MB/s eta 0:00:03\n",
      "     -----------                              3.2/11.3 MB 3.5 MB/s eta 0:00:03\n",
      "     ------------                             3.4/11.3 MB 3.6 MB/s eta 0:00:03\n",
      "     ------------                             3.6/11.3 MB 3.6 MB/s eta 0:00:03\n",
      "     -------------                            3.9/11.3 MB 3.7 MB/s eta 0:00:03\n",
      "     --------------                           4.1/11.3 MB 3.7 MB/s eta 0:00:02\n",
      "     ---------------                          4.3/11.3 MB 3.8 MB/s eta 0:00:02\n",
      "     ----------------                         4.6/11.3 MB 3.8 MB/s eta 0:00:02\n",
      "     ----------------                         4.8/11.3 MB 3.9 MB/s eta 0:00:02\n",
      "     -----------------                        5.0/11.3 MB 3.9 MB/s eta 0:00:02\n",
      "     ------------------                       5.3/11.3 MB 4.0 MB/s eta 0:00:02\n",
      "     -------------------                      5.5/11.3 MB 4.0 MB/s eta 0:00:02\n",
      "     --------------------                     5.8/11.3 MB 4.0 MB/s eta 0:00:02\n",
      "     ---------------------                    6.0/11.3 MB 4.1 MB/s eta 0:00:02\n",
      "     ---------------------                    6.2/11.3 MB 4.1 MB/s eta 0:00:02\n",
      "     ----------------------                   6.5/11.3 MB 4.1 MB/s eta 0:00:02\n",
      "     -----------------------                  6.7/11.3 MB 4.2 MB/s eta 0:00:02\n",
      "     ------------------------                 7.0/11.3 MB 4.2 MB/s eta 0:00:02\n",
      "     -------------------------                7.2/11.3 MB 4.2 MB/s eta 0:00:01\n",
      "     --------------------------               7.5/11.3 MB 4.3 MB/s eta 0:00:01\n",
      "     ---------------------------              7.7/11.3 MB 4.3 MB/s eta 0:00:01\n",
      "     ----------------------------             8.0/11.3 MB 4.3 MB/s eta 0:00:01\n",
      "     ----------------------------             8.2/11.3 MB 4.4 MB/s eta 0:00:01\n",
      "     -----------------------------            8.5/11.3 MB 4.4 MB/s eta 0:00:01\n",
      "     ------------------------------           8.7/11.3 MB 4.4 MB/s eta 0:00:01\n",
      "     -------------------------------          9.0/11.3 MB 4.4 MB/s eta 0:00:01\n",
      "     --------------------------------         9.2/11.3 MB 4.5 MB/s eta 0:00:01\n",
      "     ---------------------------------        9.5/11.3 MB 4.5 MB/s eta 0:00:01\n",
      "     ----------------------------------       9.7/11.3 MB 4.5 MB/s eta 0:00:01\n",
      "     -----------------------------------      10.0/11.3 MB 4.5 MB/s eta 0:00:01\n",
      "     ------------------------------------     10.3/11.3 MB 4.6 MB/s eta 0:00:01\n",
      "     -------------------------------------    10.5/11.3 MB 4.8 MB/s eta 0:00:01\n",
      "     --------------------------------------   10.8/11.3 MB 5.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  11.1/11.3 MB 5.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  11.3/11.3 MB 5.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 11.3/11.3 MB 5.1 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Downloading numpy-2.3.1-cp311-cp311-win_amd64.whl (13.0 MB)\n",
      "                                              0.0/13.0 MB ? eta -:--:--\n",
      "                                              0.2/13.0 MB 5.8 MB/s eta 0:00:03\n",
      "     -                                        0.5/13.0 MB 6.0 MB/s eta 0:00:03\n",
      "     --                                       0.7/13.0 MB 5.8 MB/s eta 0:00:03\n",
      "     ---                                      1.0/13.0 MB 5.9 MB/s eta 0:00:03\n",
      "     ---                                      1.3/13.0 MB 5.8 MB/s eta 0:00:03\n",
      "     ----                                     1.6/13.0 MB 5.9 MB/s eta 0:00:02\n",
      "     -----                                    1.8/13.0 MB 5.9 MB/s eta 0:00:02\n",
      "     ------                                   2.1/13.0 MB 5.9 MB/s eta 0:00:02\n",
      "     -------                                  2.4/13.0 MB 6.1 MB/s eta 0:00:02\n",
      "     --------                                 2.7/13.0 MB 6.1 MB/s eta 0:00:02\n",
      "     ---------                                3.0/13.0 MB 6.1 MB/s eta 0:00:02\n",
      "     ----------                               3.3/13.0 MB 6.0 MB/s eta 0:00:02\n",
      "     -----------                              3.6/13.0 MB 6.0 MB/s eta 0:00:02\n",
      "     -----------                              3.9/13.0 MB 6.1 MB/s eta 0:00:02\n",
      "     ------------                             4.2/13.0 MB 6.1 MB/s eta 0:00:02\n",
      "     -------------                            4.5/13.0 MB 6.1 MB/s eta 0:00:02\n",
      "     --------------                           4.8/13.0 MB 6.2 MB/s eta 0:00:02\n",
      "     ---------------                          5.1/13.0 MB 6.1 MB/s eta 0:00:02\n",
      "     ----------------                         5.4/13.0 MB 6.1 MB/s eta 0:00:02\n",
      "     -----------------                        5.7/13.0 MB 6.2 MB/s eta 0:00:02\n",
      "     ------------------                       6.0/13.0 MB 6.3 MB/s eta 0:00:02\n",
      "     -------------------                      6.3/13.0 MB 6.3 MB/s eta 0:00:02\n",
      "     --------------------                     6.6/13.0 MB 6.3 MB/s eta 0:00:02\n",
      "     ---------------------                    6.9/13.0 MB 6.3 MB/s eta 0:00:01\n",
      "     ----------------------                   7.2/13.0 MB 6.3 MB/s eta 0:00:01\n",
      "     -----------------------                  7.5/13.0 MB 6.3 MB/s eta 0:00:01\n",
      "     ------------------------                 7.8/13.0 MB 6.3 MB/s eta 0:00:01\n",
      "     -------------------------                8.2/13.0 MB 6.4 MB/s eta 0:00:01\n",
      "     --------------------------               8.5/13.0 MB 6.4 MB/s eta 0:00:01\n",
      "     --------------------------               8.8/13.0 MB 6.4 MB/s eta 0:00:01\n",
      "     ---------------------------              9.1/13.0 MB 6.4 MB/s eta 0:00:01\n",
      "     ----------------------------             9.4/13.0 MB 6.4 MB/s eta 0:00:01\n",
      "     -----------------------------            9.7/13.0 MB 6.4 MB/s eta 0:00:01\n",
      "     ------------------------------           10.1/13.0 MB 6.4 MB/s eta 0:00:01\n",
      "     -------------------------------          10.4/13.0 MB 6.4 MB/s eta 0:00:01\n",
      "     --------------------------------         10.7/13.0 MB 6.5 MB/s eta 0:00:01\n",
      "     ----------------------------------       11.1/13.0 MB 6.5 MB/s eta 0:00:01\n",
      "     -----------------------------------      11.4/13.0 MB 6.5 MB/s eta 0:00:01\n",
      "     -----------------------------------      11.7/13.0 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------------------    12.1/13.0 MB 6.7 MB/s eta 0:00:01\n",
      "     --------------------------------------   12.4/13.0 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.7/13.0 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  13.0/13.0 MB 6.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 13.0/13.0 MB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\kagle\\meta_kaggle_hackathon\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "                                              0.0/509.2 kB ? eta -:--:--\n",
      "     -------------------                    266.2/509.2 kB 8.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 509.2/509.2 kB 6.4 MB/s eta 0:00:00\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "                                              0.0/347.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 347.8/347.8 kB 7.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\kagle\\meta_kaggle_hackathon\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.3.1 pandas-2.3.1 pytz-2025.2 tzdata-2025.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12d32713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e70c8d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\kagle\\meta_kaggle_hackathon\\venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: pyarrow in c:\\kagle\\meta_kaggle_hackathon\\venv\\lib\\site-packages (21.0.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\kagle\\meta_kaggle_hackathon\\venv\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\kagle\\meta_kaggle_hackathon\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\kagle\\meta_kaggle_hackathon\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\kagle\\meta_kaggle_hackathon\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\kagle\\meta_kaggle_hackathon\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pandas pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e03d64a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"C:\\Kagle\\Meta_Kaggle_Hackathon\\data\\drw-crypto-market-prediction\\\\test.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "adb8ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "train_df = pq.read_table(source=FILE_PATH).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a5ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bid_qty</th>\n",
       "      <th>ask_qty</th>\n",
       "      <th>buy_qty</th>\n",
       "      <th>sell_qty</th>\n",
       "      <th>volume</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>...</th>\n",
       "      <th>X772</th>\n",
       "      <th>X773</th>\n",
       "      <th>X774</th>\n",
       "      <th>X775</th>\n",
       "      <th>X776</th>\n",
       "      <th>X777</th>\n",
       "      <th>X778</th>\n",
       "      <th>X779</th>\n",
       "      <th>X780</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58518</th>\n",
       "      <td>2.048</td>\n",
       "      <td>13.761</td>\n",
       "      <td>398.468</td>\n",
       "      <td>483.119</td>\n",
       "      <td>881.587</td>\n",
       "      <td>3.010245</td>\n",
       "      <td>1.119424</td>\n",
       "      <td>2.255157</td>\n",
       "      <td>2.779649</td>\n",
       "      <td>2.918978</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.132445</td>\n",
       "      <td>-0.008857</td>\n",
       "      <td>-0.703392</td>\n",
       "      <td>-0.717460</td>\n",
       "      <td>-0.731671</td>\n",
       "      <td>-0.751135</td>\n",
       "      <td>-0.798345</td>\n",
       "      <td>-0.855403</td>\n",
       "      <td>-0.892099</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371509</th>\n",
       "      <td>1.352</td>\n",
       "      <td>4.910</td>\n",
       "      <td>13.202</td>\n",
       "      <td>55.164</td>\n",
       "      <td>68.366</td>\n",
       "      <td>0.430031</td>\n",
       "      <td>1.264623</td>\n",
       "      <td>1.275600</td>\n",
       "      <td>1.105269</td>\n",
       "      <td>0.912849</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102024</td>\n",
       "      <td>-0.028531</td>\n",
       "      <td>-0.664251</td>\n",
       "      <td>-0.273688</td>\n",
       "      <td>-0.087196</td>\n",
       "      <td>-0.172973</td>\n",
       "      <td>-0.469420</td>\n",
       "      <td>-0.406631</td>\n",
       "      <td>-0.019688</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336495</th>\n",
       "      <td>3.582</td>\n",
       "      <td>4.163</td>\n",
       "      <td>48.771</td>\n",
       "      <td>35.786</td>\n",
       "      <td>84.557</td>\n",
       "      <td>1.435067</td>\n",
       "      <td>0.843794</td>\n",
       "      <td>1.387360</td>\n",
       "      <td>1.383508</td>\n",
       "      <td>1.356717</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.197316</td>\n",
       "      <td>-0.068577</td>\n",
       "      <td>-0.703392</td>\n",
       "      <td>-0.717434</td>\n",
       "      <td>-0.731608</td>\n",
       "      <td>-0.749504</td>\n",
       "      <td>-0.765106</td>\n",
       "      <td>-0.820837</td>\n",
       "      <td>-0.946519</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297839</th>\n",
       "      <td>6.554</td>\n",
       "      <td>6.707</td>\n",
       "      <td>26.526</td>\n",
       "      <td>59.988</td>\n",
       "      <td>86.514</td>\n",
       "      <td>-0.349729</td>\n",
       "      <td>0.638728</td>\n",
       "      <td>0.613413</td>\n",
       "      <td>0.554099</td>\n",
       "      <td>0.514814</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131409</td>\n",
       "      <td>-0.014809</td>\n",
       "      <td>-0.702295</td>\n",
       "      <td>-0.646356</td>\n",
       "      <td>-0.329237</td>\n",
       "      <td>0.236389</td>\n",
       "      <td>1.108468</td>\n",
       "      <td>1.552691</td>\n",
       "      <td>1.861040</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150513</th>\n",
       "      <td>1.174</td>\n",
       "      <td>9.461</td>\n",
       "      <td>56.319</td>\n",
       "      <td>33.325</td>\n",
       "      <td>89.644</td>\n",
       "      <td>0.756325</td>\n",
       "      <td>0.142690</td>\n",
       "      <td>0.443266</td>\n",
       "      <td>0.670515</td>\n",
       "      <td>0.827578</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.230074</td>\n",
       "      <td>-0.079070</td>\n",
       "      <td>-0.702793</td>\n",
       "      <td>-0.701907</td>\n",
       "      <td>-0.581251</td>\n",
       "      <td>-0.259967</td>\n",
       "      <td>0.336438</td>\n",
       "      <td>0.534184</td>\n",
       "      <td>0.505956</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 786 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        bid_qty  ask_qty  buy_qty  sell_qty   volume        X1        X2  \\\n",
       "ID                                                                         \n",
       "58518     2.048   13.761  398.468   483.119  881.587  3.010245  1.119424   \n",
       "371509    1.352    4.910   13.202    55.164   68.366  0.430031  1.264623   \n",
       "336495    3.582    4.163   48.771    35.786   84.557  1.435067  0.843794   \n",
       "297839    6.554    6.707   26.526    59.988   86.514 -0.349729  0.638728   \n",
       "150513    1.174    9.461   56.319    33.325   89.644  0.756325  0.142690   \n",
       "\n",
       "              X3        X4        X5  ...      X772      X773      X774  \\\n",
       "ID                                    ...                                 \n",
       "58518   2.255157  2.779649  2.918978  ... -0.132445 -0.008857 -0.703392   \n",
       "371509  1.275600  1.105269  0.912849  ... -0.102024 -0.028531 -0.664251   \n",
       "336495  1.387360  1.383508  1.356717  ... -0.197316 -0.068577 -0.703392   \n",
       "297839  0.613413  0.554099  0.514814  ... -0.131409 -0.014809 -0.702295   \n",
       "150513  0.443266  0.670515  0.827578  ... -0.230074 -0.079070 -0.702793   \n",
       "\n",
       "            X775      X776      X777      X778      X779      X780  label  \n",
       "ID                                                                         \n",
       "58518  -0.717460 -0.731671 -0.751135 -0.798345 -0.855403 -0.892099      0  \n",
       "371509 -0.273688 -0.087196 -0.172973 -0.469420 -0.406631 -0.019688      0  \n",
       "336495 -0.717434 -0.731608 -0.749504 -0.765106 -0.820837 -0.946519      0  \n",
       "297839 -0.646356 -0.329237  0.236389  1.108468  1.552691  1.861040      0  \n",
       "150513 -0.701907 -0.581251 -0.259967  0.336438  0.534184  0.505956      0  \n",
       "\n",
       "[5 rows x 786 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(5).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13099203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'X1', 'X2', 'X3',\n",
       "       'X4', 'X5',\n",
       "       ...\n",
       "       'X772', 'X773', 'X774', 'X775', 'X776', 'X777', 'X778', 'X779', 'X780',\n",
       "       'label'],\n",
       "      dtype='object', length=786)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ce6c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bid_qty</th>\n",
       "      <th>ask_qty</th>\n",
       "      <th>buy_qty</th>\n",
       "      <th>sell_qty</th>\n",
       "      <th>volume</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>...</th>\n",
       "      <th>X772</th>\n",
       "      <th>X773</th>\n",
       "      <th>X774</th>\n",
       "      <th>X775</th>\n",
       "      <th>X776</th>\n",
       "      <th>X777</th>\n",
       "      <th>X778</th>\n",
       "      <th>X779</th>\n",
       "      <th>X780</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>538150.000000</td>\n",
       "      <td>538150.000000</td>\n",
       "      <td>538150.000000</td>\n",
       "      <td>538150.000000</td>\n",
       "      <td>538150.000000</td>\n",
       "      <td>538150.000000</td>\n",
       "      <td>538150.000000</td>\n",
       "      <td>538150.000000</td>\n",
       "      <td>538150.000000</td>\n",
       "      <td>538150.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>538150.000000</td>\n",
       "      <td>538150.000000</td>\n",
       "      <td>538150.000000</td>\n",
       "      <td>538150.000000</td>\n",
       "      <td>538150.000000</td>\n",
       "      <td>538150.000000</td>\n",
       "      <td>538150.000000</td>\n",
       "      <td>538150.000000</td>\n",
       "      <td>538150.000000</td>\n",
       "      <td>538150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.633091</td>\n",
       "      <td>6.760688</td>\n",
       "      <td>85.002532</td>\n",
       "      <td>86.168611</td>\n",
       "      <td>171.171143</td>\n",
       "      <td>0.028764</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.142978</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.004068</td>\n",
       "      <td>0.004123</td>\n",
       "      <td>0.004139</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.004166</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.420952</td>\n",
       "      <td>12.221560</td>\n",
       "      <td>158.855006</td>\n",
       "      <td>165.032753</td>\n",
       "      <td>303.461795</td>\n",
       "      <td>1.069922</td>\n",
       "      <td>1.050045</td>\n",
       "      <td>1.053741</td>\n",
       "      <td>1.056637</td>\n",
       "      <td>1.059120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862474</td>\n",
       "      <td>1.165821</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.998880</td>\n",
       "      <td>0.998977</td>\n",
       "      <td>0.998939</td>\n",
       "      <td>0.998167</td>\n",
       "      <td>0.996849</td>\n",
       "      <td>0.995178</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.091443</td>\n",
       "      <td>-9.211782</td>\n",
       "      <td>-6.981692</td>\n",
       "      <td>-6.465466</td>\n",
       "      <td>-5.918329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246373</td>\n",
       "      <td>-0.171764</td>\n",
       "      <td>-0.703564</td>\n",
       "      <td>-0.717570</td>\n",
       "      <td>-0.731751</td>\n",
       "      <td>-0.751275</td>\n",
       "      <td>-0.807985</td>\n",
       "      <td>-0.889442</td>\n",
       "      <td>-1.072060</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.810000</td>\n",
       "      <td>1.849000</td>\n",
       "      <td>18.681000</td>\n",
       "      <td>19.297000</td>\n",
       "      <td>43.127000</td>\n",
       "      <td>-0.595040</td>\n",
       "      <td>-0.518693</td>\n",
       "      <td>-0.583148</td>\n",
       "      <td>-0.620918</td>\n",
       "      <td>-0.647847</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.219511</td>\n",
       "      <td>-0.054476</td>\n",
       "      <td>-0.703412</td>\n",
       "      <td>-0.717392</td>\n",
       "      <td>-0.731526</td>\n",
       "      <td>-0.749971</td>\n",
       "      <td>-0.758654</td>\n",
       "      <td>-0.791035</td>\n",
       "      <td>-0.880027</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.556500</td>\n",
       "      <td>4.630000</td>\n",
       "      <td>39.544000</td>\n",
       "      <td>40.626000</td>\n",
       "      <td>85.023000</td>\n",
       "      <td>0.025818</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>-0.001727</td>\n",
       "      <td>-0.005404</td>\n",
       "      <td>-0.011242</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156298</td>\n",
       "      <td>-0.029307</td>\n",
       "      <td>-0.702918</td>\n",
       "      <td>-0.716397</td>\n",
       "      <td>-0.724460</td>\n",
       "      <td>-0.687828</td>\n",
       "      <td>-0.599708</td>\n",
       "      <td>-0.591877</td>\n",
       "      <td>-0.396052</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.701000</td>\n",
       "      <td>8.873000</td>\n",
       "      <td>88.456000</td>\n",
       "      <td>89.803000</td>\n",
       "      <td>180.310000</td>\n",
       "      <td>0.664218</td>\n",
       "      <td>0.517635</td>\n",
       "      <td>0.575237</td>\n",
       "      <td>0.609696</td>\n",
       "      <td>0.634623</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134100</td>\n",
       "      <td>-0.015027</td>\n",
       "      <td>1.366721</td>\n",
       "      <td>1.393274</td>\n",
       "      <td>1.297756</td>\n",
       "      <td>1.284464</td>\n",
       "      <td>0.985123</td>\n",
       "      <td>0.902707</td>\n",
       "      <td>0.821145</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>929.363000</td>\n",
       "      <td>2255.094000</td>\n",
       "      <td>8508.998000</td>\n",
       "      <td>10534.646000</td>\n",
       "      <td>15351.580000</td>\n",
       "      <td>5.746226</td>\n",
       "      <td>9.983280</td>\n",
       "      <td>8.699757</td>\n",
       "      <td>7.738598</td>\n",
       "      <td>6.454116</td>\n",
       "      <td>...</td>\n",
       "      <td>164.361177</td>\n",
       "      <td>208.190544</td>\n",
       "      <td>1.523340</td>\n",
       "      <td>1.553988</td>\n",
       "      <td>1.585093</td>\n",
       "      <td>1.628151</td>\n",
       "      <td>1.765866</td>\n",
       "      <td>1.944697</td>\n",
       "      <td>2.094047</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 786 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             bid_qty        ask_qty        buy_qty       sell_qty  \\\n",
       "count  538150.000000  538150.000000  538150.000000  538150.000000   \n",
       "mean        6.633091       6.760688      85.002532      86.168611   \n",
       "std        10.420952      12.221560     158.855006     165.032753   \n",
       "min         0.001000       0.001000       0.000000       0.000000   \n",
       "25%         1.810000       1.849000      18.681000      19.297000   \n",
       "50%         4.556500       4.630000      39.544000      40.626000   \n",
       "75%         8.701000       8.873000      88.456000      89.803000   \n",
       "max       929.363000    2255.094000    8508.998000   10534.646000   \n",
       "\n",
       "              volume             X1             X2             X3  \\\n",
       "count  538150.000000  538150.000000  538150.000000  538150.000000   \n",
       "mean      171.171143       0.028764       0.000141       0.000095   \n",
       "std       303.461795       1.069922       1.050045       1.053741   \n",
       "min         0.000000      -5.091443      -9.211782      -6.981692   \n",
       "25%        43.127000      -0.595040      -0.518693      -0.583148   \n",
       "50%        85.023000       0.025818       0.001432      -0.001727   \n",
       "75%       180.310000       0.664218       0.517635       0.575237   \n",
       "max     15351.580000       5.746226       9.983280       8.699757   \n",
       "\n",
       "                  X4             X5  ...           X772           X773  \\\n",
       "count  538150.000000  538150.000000  ...  538150.000000  538150.000000   \n",
       "mean        0.000104       0.000074  ...      -0.142978       0.001410   \n",
       "std         1.056637       1.059120  ...       0.862474       1.165821   \n",
       "min        -6.465466      -5.918329  ...      -0.246373      -0.171764   \n",
       "25%        -0.620918      -0.647847  ...      -0.219511      -0.054476   \n",
       "50%        -0.005404      -0.011242  ...      -0.156298      -0.029307   \n",
       "75%         0.609696       0.634623  ...      -0.134100      -0.015027   \n",
       "max         7.738598       6.454116  ...     164.361177     208.190544   \n",
       "\n",
       "                X774           X775           X776           X777  \\\n",
       "count  538150.000000  538150.000000  538150.000000  538150.000000   \n",
       "mean        0.004068       0.004123       0.004139       0.004140   \n",
       "std         0.998707       0.998880       0.998977       0.998939   \n",
       "min        -0.703564      -0.717570      -0.731751      -0.751275   \n",
       "25%        -0.703412      -0.717392      -0.731526      -0.749971   \n",
       "50%        -0.702918      -0.716397      -0.724460      -0.687828   \n",
       "75%         1.366721       1.393274       1.297756       1.284464   \n",
       "max         1.523340       1.553988       1.585093       1.628151   \n",
       "\n",
       "                X778           X779           X780     label  \n",
       "count  538150.000000  538150.000000  538150.000000  538150.0  \n",
       "mean        0.004166       0.004273       0.004393       0.0  \n",
       "std         0.998167       0.996849       0.995178       0.0  \n",
       "min        -0.807985      -0.889442      -1.072060       0.0  \n",
       "25%        -0.758654      -0.791035      -0.880027       0.0  \n",
       "50%        -0.599708      -0.591877      -0.396052       0.0  \n",
       "75%         0.985123       0.902707       0.821145       0.0  \n",
       "max         1.765866       1.944697       2.094047       0.0  \n",
       "\n",
       "[8 rows x 786 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3625f947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\kagle\\meta_kaggle_hackathon\\venv\\lib\\site-packages (3.10.3)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-win_amd64.whl (8.9 MB)\n",
      "                                              0.0/8.9 MB ? eta -:--:--\n",
      "                                              0.1/8.9 MB 2.6 MB/s eta 0:00:04\n",
      "     -                                        0.3/8.9 MB 2.6 MB/s eta 0:00:04\n",
      "     ---                                      0.7/8.9 MB 5.4 MB/s eta 0:00:02\n",
      "     -----                                    1.2/8.9 MB 7.0 MB/s eta 0:00:02\n",
      "     -------                                  1.6/8.9 MB 7.6 MB/s eta 0:00:01\n",
      "     -------                                  1.7/8.9 MB 6.3 MB/s eta 0:00:02\n",
      "     ------------                             2.8/8.9 MB 9.1 MB/s eta 0:00:01\n",
      "     ---------------                          3.4/8.9 MB 9.5 MB/s eta 0:00:01\n",
      "     -----------------                        3.9/8.9 MB 9.6 MB/s eta 0:00:01\n",
      "     -------------------                      4.4/8.9 MB 9.6 MB/s eta 0:00:01\n",
      "     ---------------------                    4.8/8.9 MB 9.7 MB/s eta 0:00:01\n",
      "     -----------------------                  5.3/8.9 MB 9.6 MB/s eta 0:00:01\n",
      "     -------------------------                5.7/8.9 MB 9.7 MB/s eta 0:00:01\n",
      "     ---------------------------              6.2/8.9 MB 9.7 MB/s eta 0:00:01\n",
      "     -----------------------------            6.7/8.9 MB 9.7 MB/s eta 0:00:01\n",
      "     -------------------------------          7.1/8.9 MB 9.6 MB/s eta 0:00:01\n",
      "     ---------------------------------        7.5/8.9 MB 9.6 MB/s eta 0:00:01\n",
      "     -----------------------------------      8.0/8.9 MB 9.6 MB/s eta 0:00:01\n",
      "     -------------------------------------    8.4/8.9 MB 9.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  8.9/8.9 MB 9.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 8.9/8.9 MB 9.3 MB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement warnings (from versions: none)\n",
      "ERROR: No matching distribution found for warnings\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c197b78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.1-cp311-cp311-win_amd64.whl (8.9 MB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\kagle\\meta_kaggle_hackathon\\venv\\lib\\site-packages (from scikit-learn) (2.3.1)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.0-cp311-cp311-win_amd64.whl (38.6 MB)\n",
      "                                              0.0/38.6 MB ? eta -:--:--\n",
      "                                              0.1/38.6 MB 1.7 MB/s eta 0:00:23\n",
      "                                              0.2/38.6 MB 2.3 MB/s eta 0:00:17\n",
      "                                              0.7/38.6 MB 4.6 MB/s eta 0:00:09\n",
      "     -                                        1.2/38.6 MB 6.2 MB/s eta 0:00:07\n",
      "     -                                        1.4/38.6 MB 7.1 MB/s eta 0:00:06\n",
      "     -                                        1.8/38.6 MB 6.3 MB/s eta 0:00:06\n",
      "     --                                       2.8/38.6 MB 8.6 MB/s eta 0:00:05\n",
      "     ---                                      3.4/38.6 MB 8.9 MB/s eta 0:00:04\n",
      "     ----                                     3.9/38.6 MB 9.1 MB/s eta 0:00:04\n",
      "     ----                                     4.3/38.6 MB 9.2 MB/s eta 0:00:04\n",
      "     ----                                     4.8/38.6 MB 9.2 MB/s eta 0:00:04\n",
      "     -----                                    5.2/38.6 MB 9.3 MB/s eta 0:00:04\n",
      "     -----                                    5.7/38.6 MB 9.5 MB/s eta 0:00:04\n",
      "     ------                                   6.1/38.6 MB 9.5 MB/s eta 0:00:04\n",
      "     ------                                   6.6/38.6 MB 9.5 MB/s eta 0:00:04\n",
      "     -------                                  7.0/38.6 MB 9.5 MB/s eta 0:00:04\n",
      "     -------                                  7.5/38.6 MB 9.5 MB/s eta 0:00:04\n",
      "     --------                                 7.9/38.6 MB 9.5 MB/s eta 0:00:04\n",
      "     --------                                 8.3/38.6 MB 9.5 MB/s eta 0:00:04\n",
      "     ---------                                8.8/38.6 MB 9.5 MB/s eta 0:00:04\n",
      "     ---------                                9.2/38.6 MB 9.5 MB/s eta 0:00:04\n",
      "     ----------                               9.7/38.6 MB 9.5 MB/s eta 0:00:04\n",
      "     ----------                               10.1/38.6 MB 9.5 MB/s eta 0:00:03\n",
      "     ----------                              10.6/38.6 MB 10.2 MB/s eta 0:00:03\n",
      "     -----------                             11.1/38.6 MB 10.2 MB/s eta 0:00:03\n",
      "     -----------                             11.5/38.6 MB 10.1 MB/s eta 0:00:03\n",
      "     ------------                            12.0/38.6 MB 10.6 MB/s eta 0:00:03\n",
      "     ------------                            12.4/38.6 MB 10.2 MB/s eta 0:00:03\n",
      "     -------------                            12.9/38.6 MB 9.9 MB/s eta 0:00:03\n",
      "     -------------                            13.4/38.6 MB 9.8 MB/s eta 0:00:03\n",
      "     --------------                           13.8/38.6 MB 9.8 MB/s eta 0:00:03\n",
      "     --------------                           14.3/38.6 MB 9.8 MB/s eta 0:00:03\n",
      "     ---------------                          14.7/38.6 MB 9.8 MB/s eta 0:00:03\n",
      "     ---------------                          15.2/38.6 MB 9.8 MB/s eta 0:00:03\n",
      "     ----------------                         15.6/38.6 MB 9.8 MB/s eta 0:00:03\n",
      "     ----------------                         16.1/38.6 MB 9.8 MB/s eta 0:00:03\n",
      "     -----------------                        16.6/38.6 MB 9.8 MB/s eta 0:00:03\n",
      "     -----------------                        17.0/38.6 MB 9.8 MB/s eta 0:00:03\n",
      "     ------------------                       17.4/38.6 MB 9.6 MB/s eta 0:00:03\n",
      "     ------------------                       17.4/38.6 MB 9.6 MB/s eta 0:00:03\n",
      "     ------------------                       17.4/38.6 MB 9.6 MB/s eta 0:00:03\n",
      "     ------------------                       18.1/38.6 MB 9.1 MB/s eta 0:00:03\n",
      "     ------------------                       18.2/38.6 MB 9.0 MB/s eta 0:00:03\n",
      "     -------------------                      19.0/38.6 MB 9.1 MB/s eta 0:00:03\n",
      "     -------------------                      19.3/38.6 MB 9.0 MB/s eta 0:00:03\n",
      "     --------------------                     19.7/38.6 MB 9.0 MB/s eta 0:00:03\n",
      "     --------------------                     20.0/38.6 MB 8.8 MB/s eta 0:00:03\n",
      "     ---------------------                    20.4/38.6 MB 8.8 MB/s eta 0:00:03\n",
      "     ---------------------                    20.7/38.6 MB 8.6 MB/s eta 0:00:03\n",
      "     ---------------------                    21.2/38.6 MB 8.6 MB/s eta 0:00:03\n",
      "     ----------------------                   21.6/38.6 MB 8.7 MB/s eta 0:00:02\n",
      "     ----------------------                   22.0/38.6 MB 8.6 MB/s eta 0:00:02\n",
      "     -----------------------                  22.5/38.6 MB 8.6 MB/s eta 0:00:02\n",
      "     -----------------------                  22.9/38.6 MB 8.6 MB/s eta 0:00:02\n",
      "     ------------------------                 23.3/38.6 MB 8.5 MB/s eta 0:00:02\n",
      "     ------------------------                 23.8/38.6 MB 8.5 MB/s eta 0:00:02\n",
      "     -------------------------                24.2/38.6 MB 8.5 MB/s eta 0:00:02\n",
      "     -------------------------                24.7/38.6 MB 8.5 MB/s eta 0:00:02\n",
      "     --------------------------               25.1/38.6 MB 8.5 MB/s eta 0:00:02\n",
      "     --------------------------               25.5/38.6 MB 8.6 MB/s eta 0:00:02\n",
      "     --------------------------               25.9/38.6 MB 8.5 MB/s eta 0:00:02\n",
      "     ---------------------------              26.4/38.6 MB 8.5 MB/s eta 0:00:02\n",
      "     ---------------------------              26.8/38.6 MB 8.5 MB/s eta 0:00:02\n",
      "     ----------------------------             27.3/38.6 MB 8.5 MB/s eta 0:00:02\n",
      "     ----------------------------             27.7/38.6 MB 9.4 MB/s eta 0:00:02\n",
      "     -----------------------------            28.2/38.6 MB 9.1 MB/s eta 0:00:02\n",
      "     -----------------------------            28.6/38.6 MB 9.2 MB/s eta 0:00:02\n",
      "     ------------------------------           29.1/38.6 MB 9.1 MB/s eta 0:00:02\n",
      "     ------------------------------           29.5/38.6 MB 9.2 MB/s eta 0:00:01\n",
      "     -------------------------------          30.0/38.6 MB 9.2 MB/s eta 0:00:01\n",
      "     -------------------------------          30.4/38.6 MB 9.4 MB/s eta 0:00:01\n",
      "     --------------------------------         30.9/38.6 MB 9.5 MB/s eta 0:00:01\n",
      "     --------------------------------         31.4/38.6 MB 9.5 MB/s eta 0:00:01\n",
      "     ---------------------------------        31.8/38.6 MB 9.5 MB/s eta 0:00:01\n",
      "     ---------------------------------        32.3/38.6 MB 9.6 MB/s eta 0:00:01\n",
      "     ---------------------------------        32.7/38.6 MB 9.6 MB/s eta 0:00:01\n",
      "     ----------------------------------       33.1/38.6 MB 9.6 MB/s eta 0:00:01\n",
      "     ----------------------------------       33.6/38.6 MB 9.6 MB/s eta 0:00:01\n",
      "     -----------------------------------      34.1/38.6 MB 9.6 MB/s eta 0:00:01\n",
      "     -----------------------------------      34.5/38.6 MB 9.6 MB/s eta 0:00:01\n",
      "     ------------------------------------     35.0/38.6 MB 9.6 MB/s eta 0:00:01\n",
      "     ------------------------------------     35.4/38.6 MB 9.6 MB/s eta 0:00:01\n",
      "     -------------------------------------    35.7/38.6 MB 9.6 MB/s eta 0:00:01\n",
      "     -------------------------------------    36.1/38.6 MB 9.5 MB/s eta 0:00:01\n",
      "     -------------------------------------    36.4/38.6 MB 9.4 MB/s eta 0:00:01\n",
      "     --------------------------------------   36.9/38.6 MB 9.4 MB/s eta 0:00:01\n",
      "     --------------------------------------   37.2/38.6 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  37.7/38.6 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.0/38.6 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.3/38.6 MB 9.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.6/38.6 MB 9.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.6/38.6 MB 9.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 38.6/38.6 MB 8.3 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "                                              0.0/307.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 307.7/307.7 kB 9.6 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.1 scipy-1.16.0 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52128e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ULTRA-ENHANCED CRYPTO PREDICTION WITH COMPLETE FEATURE SET\n",
      "======================================================================\n",
      "Memory usage before optimization: 3227.13 MB\n",
      "Memory usage after optimization: 1612.02 MB\n",
      "Decreased by 50.0%\n",
      "Creating advanced engineered features...\n",
      "Found 799 features out of 816 requested\n",
      "Memory usage before optimization: 1646.41 MB\n",
      "Memory usage after optimization: 1640.25 MB\n",
      "Decreased by 0.4%\n",
      "Training Enhanced Incremental Lag Ensemble...\n",
      "Total epochs per model: 20\n",
      "Split 799 features into 40 batches\n",
      "Total lag strategies: 21\n",
      "Total unique lag values: 75\n",
      "\n",
      "Training micro strategy (lags: [1, 2, 3, 4, 5])\n",
      "Progress: 1/840 models\n",
      "\n",
      "  Processing feature batch (20 features) with micro lags\n",
      "  Training SGD model: micro_X727_X501 (20 epochs)\n",
      "    Epoch 1/20 completed\n",
      "    Epoch 5/20 completed\n",
      "    Epoch 9/20 completed\n",
      "    Epoch 13/20 completed\n",
      "    Epoch 17/20 completed\n",
      "Progress: 2/840 models\n",
      "\n",
      "  Processing feature batch (20 features) with micro lags\n",
      "  Training SGD model: micro_X303_X518 (20 epochs)\n",
      "    Epoch 1/20 completed\n",
      "    Epoch 5/20 completed\n",
      "    Epoch 9/20 completed\n",
      "    Epoch 13/20 completed\n",
      "    Epoch 17/20 completed\n",
      "Progress: 3/840 models\n",
      "\n",
      "  Processing feature batch (20 features) with micro lags\n",
      "  Training SGD model: micro_X502_X35 (20 epochs)\n",
      "    Epoch 1/20 completed\n",
      "    Epoch 5/20 completed\n",
      "    Epoch 9/20 completed\n",
      "    Epoch 13/20 completed\n",
      "    Epoch 17/20 completed\n",
      "Progress: 4/840 models\n",
      "\n",
      "  Processing feature batch (20 features) with micro lags\n",
      "  Training SGD model: micro_X128_X707 (20 epochs)\n",
      "    Epoch 1/20 completed\n",
      "    Epoch 5/20 completed\n",
      "    Epoch 9/20 completed\n",
      "    Epoch 13/20 completed\n",
      "    Epoch 17/20 completed\n",
      "Progress: 5/840 models\n",
      "\n",
      "  Processing feature batch (20 features) with micro lags\n",
      "  Training SGD model: micro_X653_X701 (20 epochs)\n",
      "    Epoch 1/20 completed\n",
      "    Epoch 5/20 completed\n",
      "    Epoch 9/20 completed\n",
      "    Epoch 13/20 completed\n",
      "    Epoch 17/20 completed\n",
      "Progress: 6/840 models\n",
      "\n",
      "  Processing feature batch (20 features) with micro lags\n",
      "  Training SGD model: micro_X725_X436 (20 epochs)\n",
      "    Epoch 1/20 completed\n",
      "    Epoch 5/20 completed\n",
      "    Epoch 9/20 completed\n",
      "    Epoch 13/20 completed\n",
      "    Epoch 17/20 completed\n",
      "Progress: 7/840 models\n",
      "\n",
      "  Processing feature batch (20 features) with micro lags\n",
      "  Training SGD model: micro_X305_X199 (20 epochs)\n",
      "    Epoch 1/20 completed\n",
      "    Epoch 5/20 completed\n",
      "    Epoch 9/20 completed\n",
      "    Epoch 13/20 completed\n",
      "    Epoch 17/20 completed\n",
      "Progress: 8/840 models\n",
      "\n",
      "  Processing feature batch (20 features) with micro lags\n",
      "  Training SGD model: micro_X18_X693 (20 epochs)\n",
      "    Epoch 1/20 completed\n",
      "    Epoch 5/20 completed\n",
      "    Epoch 9/20 completed\n",
      "    Epoch 13/20 completed\n",
      "    Epoch 17/20 completed\n",
      "Progress: 9/840 models\n",
      "\n",
      "  Processing feature batch (20 features) with micro lags\n",
      "  Training SGD model: micro_X102_X587 (20 epochs)\n",
      "    Epoch 1/20 completed\n",
      "    Epoch 5/20 completed\n",
      "    Epoch 9/20 completed\n",
      "    Epoch 13/20 completed\n",
      "    Epoch 17/20 completed\n",
      "Progress: 10/840 models\n",
      "\n",
      "  Processing feature batch (20 features) with micro lags\n",
      "  Training SGD model: micro_X450_X756 (20 epochs)\n",
      "    Epoch 1/20 completed\n",
      "    Epoch 5/20 completed\n",
      "    Epoch 9/20 completed\n",
      "    Epoch 13/20 completed\n",
      "    Epoch 17/20 completed\n",
      "Progress: 11/840 models\n",
      "\n",
      "  Processing feature batch (20 features) with micro lags\n",
      "  Training SGD model: micro_X589_X105 (20 epochs)\n",
      "    Epoch 1/20 completed\n",
      "    Epoch 5/20 completed\n",
      "    Epoch 9/20 completed\n",
      "    Epoch 13/20 completed\n",
      "    Epoch 17/20 completed\n",
      "Progress: 12/840 models\n",
      "\n",
      "  Processing feature batch (20 features) with micro lags\n",
      "  Training SGD model: micro_X63_X38 (20 epochs)\n",
      "    Epoch 1/20 completed\n",
      "    Epoch 5/20 completed\n",
      "    Epoch 9/20 completed\n",
      "    Epoch 13/20 completed\n",
      "    Epoch 17/20 completed\n",
      "Progress: 13/840 models\n",
      "\n",
      "  Processing feature batch (20 features) with micro lags\n",
      "  Training SGD model: micro_X634_X401 (20 epochs)\n",
      "    Epoch 1/20 completed\n",
      "    Epoch 5/20 completed\n",
      "    Epoch 9/20 completed\n",
      "    Epoch 13/20 completed\n",
      "    Epoch 17/20 completed\n",
      "Progress: 14/840 models\n",
      "\n",
      "  Processing feature batch (20 features) with micro lags\n",
      "  Training SGD model: micro_X629_X376 (20 epochs)\n",
      "    Epoch 1/20 completed\n",
      "    Epoch 5/20 completed\n",
      "    Epoch 9/20 completed\n",
      "    Epoch 13/20 completed\n",
      "    Epoch 17/20 completed\n",
      "Progress: 15/840 models\n",
      "\n",
      "  Processing feature batch (20 features) with micro lags\n",
      "  Training SGD model: micro_X71_X12 (20 epochs)\n",
      "    Epoch 1/20 completed\n",
      "    Epoch 5/20 completed\n",
      "    Epoch 9/20 completed\n",
      "    Epoch 13/20 completed\n",
      "    Epoch 17/20 completed\n",
      "Progress: 16/840 models\n",
      "\n",
      "  Processing feature batch (20 features) with micro lags\n",
      "  Training SGD model: micro_X678_X668 (20 epochs)\n",
      "    Epoch 1/20 completed\n",
      "    Epoch 5/20 completed\n",
      "    Epoch 9/20 completed\n",
      "    Epoch 13/20 completed\n",
      "    Epoch 17/20 completed\n",
      "Progress: 17/840 models\n",
      "\n",
      "  Processing feature batch (20 features) with micro lags\n",
      "  Training SGD model: micro_X585_X166 (20 epochs)\n",
      "    Epoch 1/20 completed\n",
      "    Epoch 5/20 completed\n",
      "    Epoch 9/20 completed\n",
      "    Epoch 13/20 completed\n",
      "    Epoch 17/20 completed\n",
      "Progress: 18/840 models\n",
      "\n",
      "  Processing feature batch (20 features) with micro lags\n",
      "  Training SGD model: micro_X719_X505 (20 epochs)\n",
      "    Epoch 1/20 completed\n",
      "    Epoch 5/20 completed\n",
      "    Epoch 9/20 completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "def optimize_memory(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Optimize memory usage by downcasting numeric types where possible.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        start_mem = df.memory_usage().sum() / 1024**2\n",
    "        print(f'Memory usage before optimization: {start_mem:.2f} MB')\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != 'object':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    \n",
    "    if verbose:\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        print(f'Memory usage after optimization: {end_mem:.2f} MB')\n",
    "        print(f'Decreased by {100 * (start_mem - end_mem) / start_mem:.1f}%')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_advanced_features(df):\n",
    "    \"\"\"Create advanced engineered features for better model performance.\"\"\"\n",
    "    print(\"Creating advanced engineered features...\")\n",
    "    \n",
    "    # Ensure we have the necessary columns\n",
    "    required_cols = ['buy_qty', 'sell_qty', 'bid_qty', 'ask_qty', 'volume']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing columns for feature engineering: {missing_cols}\")\n",
    "        return df\n",
    "    \n",
    "    # Volume-based features\n",
    "    if 'volume' in df.columns:\n",
    "        df['log_volume_squared'] = np.log1p(df['volume']) ** 2\n",
    "        df['volume_sqrt'] = np.sqrt(df['volume'])\n",
    "    \n",
    "    # Order flow features\n",
    "    if all(col in df.columns for col in ['buy_qty', 'sell_qty']):\n",
    "        df['order_flow_ratio'] = df['buy_qty'] / (df['sell_qty'] + 1)\n",
    "        df['order_flow_diff'] = df['buy_qty'] - df['sell_qty']\n",
    "        df['order_flow_imbalance_squared'] = ((df['buy_qty'] - df['sell_qty']) / (df['buy_qty'] + df['sell_qty'] + 1)) ** 2\n",
    "    \n",
    "    # Spread and liquidity features\n",
    "    if all(col in df.columns for col in ['bid_qty', 'ask_qty']):\n",
    "        df['depth_imbalance_ratio'] = (df['bid_qty'] - df['ask_qty']) / (df['bid_qty'] + df['ask_qty'] + 1)\n",
    "        df['total_depth'] = df['bid_qty'] + df['ask_qty']\n",
    "        df['log_total_depth'] = np.log1p(df['total_depth'])\n",
    "    \n",
    "    # Momentum features (if price-related columns exist)\n",
    "    price_cols = [col for col in df.columns if 'price' in col.lower() or col in ['X363', 'X405', 'X321']]\n",
    "    if price_cols:\n",
    "        for col in price_cols[:3]:  # Limit to top 3 price columns\n",
    "            df[f'{col}_momentum'] = df[col].pct_change(5).fillna(0)\n",
    "            df[f'{col}_volatility'] = df[col].rolling(10).std().fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_feature_columns(df):\n",
    "    \"\"\"Get all feature columns including the extended list and new variables.\"\"\"\n",
    "    # Extended feature list provided\n",
    "    extended_features = [\n",
    "        'X727', 'X427', 'X288', 'X721', 'X312', 'X421', 'X471', 'X573', 'X780', 'X255',\n",
    "        'X144', 'X299', 'X301', 'X563', 'X737', 'X702', 'ask_qty', 'X507', 'X306', 'X501',\n",
    "        'X303', 'amihud_illiquidity', 'X586', 'X43', 'X517', 'X248', 'X137', 'X757', 'X196',\n",
    "        'X777', 'X280', 'X266', 'X689', 'X294', 'X492', 'X555', 'X731', 'X262', 'X576',\n",
    "        'X13', 'X518', 'X502', 'X558', 'pin_proxy', 'X6', 'X602', 'X695', 'X703', 'X413',\n",
    "        'X660', 'X37', 'X15', 'X310', 'X512', 'X362', 'X631', 'X214', 'X562', 'X488',\n",
    "        'X510', 'X256', 'X35', 'X128', 'X86', 'X170', 'X30', 'X265', 'X323', 'X559',\n",
    "        'X348', 'X130', 'X529', 'X20', 'X4', 'X90', 'X192', 'X91', 'X582', 'X99',\n",
    "        'X24', 'X317', 'X707', 'X653', 'X519', 'X557', 'X371', 'X415', 'X84', 'X83',\n",
    "        'order_toxicity', 'X360', 'X111', 'X699', 'X187', 'X591', 'X637', 'X567', 'X577',\n",
    "        'X313', 'X60', 'X671', 'X698', 'X701', 'X725', 'X292', 'X638', 'X741', 'X379',\n",
    "        'X700', 'X614', 'X676', 'X516', 'X697', 'X611', 'X311', 'X615', 'X706', 'X466',\n",
    "        'X571', 'X451', 'X17', 'X584', 'X436', 'X305', 'liquidity_consumption', 'X34', 'X282',\n",
    "        'X681', 'X7', 'X208', 'X41', 'X536', 'X548', 'X296', 'X776', 'X87', 'X40',\n",
    "        'X570', 'X539', 'X474', 'X753', 'X425', 'X217', 'X199', 'X18', 'X609', 'X21',\n",
    "        'X277', 'X279', 'X326', 'X540', 'X688', 'X553', 'X452', 'X738', 'X183', 'X759',\n",
    "        'bid_ask_ratio', 'X495', 'volume_participation', 'X715', 'X385', 'X291', 'X409', 'X112',\n",
    "        'X693', 'X102', 'X318', 'X705', 'X556', 'X547'\n",
    "    ]\n",
    "    \n",
    "    # Core features that are usually important\n",
    "    core_features = [\n",
    "        'X363', 'X405', 'X321', 'X175', 'X179', 'X197', 'X22', 'X181', 'X28', 'X169',\n",
    "        'X198', 'X173', 'X338', 'X344', 'X587', 'X450', 'X97', 'X52', 'X444', 'X598',\n",
    "        'X297', 'X138', 'X572', 'X343', 'X438', 'X459', 'X758', 'X25', 'buy_qty', \n",
    "        'sell_qty', 'volume', 'bid_qty'\n",
    "    ]\n",
    "    \n",
    "    # NEW features to add\n",
    "    new_features = [\n",
    "        'X363', 'X321', 'X405', 'X730', 'X523', 'X756', 'X589', 'X462', 'X779', 'log_liquidity',\n",
    "        'X25', 'X532', 'X520', 'X329', 'X383', 'X751', 'X535', 'X639', 'X596', 'X761',\n",
    "        'X145', 'X709', 'X173', 'X245', 'X168', 'X171', 'X241', 'X31', 'X105', 'X63',\n",
    "        'X263', 'X426', 'X286', 'X357', 'X399', 'X315', 'X468', 'X131', 'X647', 'log_spread',\n",
    "        'X752', 'X254', 'X592', 'X733', 'X636', 'X394', 'X527', 'X180', 'X367', 'X38',\n",
    "        'X634', 'X718', 'X387', 'X429', 'X345', 'X344', 'X253', 'X469', 'X446', 'X125',\n",
    "        'X760', 'X186', 'X711', 'X150', 'X661', 'X215', 'X403', 'X141', 'X771', 'X453',\n",
    "        'X401', 'X629', 'X616', 'X281', 'X432', 'X283', 'X244', 'X440', 'X430', 'X382',\n",
    "        'X175', 'X95', 'X444', 'X189', 'X55', 'X605', 'X663', 'X194', 'X439', 'X670',\n",
    "        'X483', 'X163', 'X376', 'X71', 'X650', 'X203', 'X8', 'X624', 'X160', 'X100',\n",
    "        'X14', 'X511', 'X59', 'X302', 'X81', 'X325', 'X514', 'X649', 'X447', 'X538',\n",
    "        'X443', 'X39', 'X343', 'X12', 'X678', 'X775', 'X498', 'X249', 'X42', 'X384',\n",
    "        'kyle_lambda', 'X349', 'X356', 'X2', 'X250', 'X397', 'X685', 'X568', 'X136', 'X496',\n",
    "        'X53', 'X66', 'X374', 'X590', 'X668', 'X585', 'X677', 'X667', 'X530', 'X28',\n",
    "        'X64', 'X407', 'X494', 'X770', 'X710', 'X526', 'X644', 'X167', 'X190', 'X723',\n",
    "        'X33', 'X579', 'X206'\n",
    "    ]\n",
    "    \n",
    "    # ADDITIONAL features requested by user (first batch)\n",
    "    additional_features = [\n",
    "        'X525', 'X267', 'X166', 'X719', 'X489', 'X758', 'X652', 'X433', 'X778', 'X428',\n",
    "        'X617', 'X259', 'X633', 'X565', 'X364', 'depth_ratio', 'X550', 'X687', 'X610', 'X599',\n",
    "        'X717', 'X587', 'X143', 'X506', 'X546', 'X505', 'X159', 'X574', 'X278', 'X458',\n",
    "        'X1', 'X749', 'X155', 'X651', 'X470', 'X580', 'X445', 'X373', 'X82', 'X607',\n",
    "        'X298', 'X221', 'X388', 'X120', 'X391', 'X23', 'X679', 'X377', 'X767', 'X755',\n",
    "        'X566', 'X424', 'X438', 'X198', 'X300', 'X268', 'X434', 'X290', 'X368', 'X464',\n",
    "        'X119', 'X197', 'X597', 'X157', 'X485', 'X127', 'X101', 'X533', 'X235', 'X712',\n",
    "        'X154', 'X239', 'X10', 'X420', 'X449', 'X740', 'X227', 'X36', 'X358', 'X551',\n",
    "        'X528', 'X285', 'X335', 'X152', 'X110', 'X68', 'X713', 'X402', 'X370', 'X735',\n",
    "        'X200', 'X331', 'X473', 'X162', 'X213', 'X322', 'X289', 'X477', 'X113', 'X560',\n",
    "        'X672', 'X621', 'X682', 'X5', 'X72', 'X44', 'X419', 'buy_pressure', 'X242', 'volume',\n",
    "        'X472', 'X332', 'X441', 'buy_sell_ratio', 'pressure_ratio', 'X508', 'X594', 'X191',\n",
    "        'X261', 'X603', 'net_pressure', 'order_flow_imbalance', 'sell_pressure', 'X240', 'X673',\n",
    "        'X608', 'X509', 'X165', 'X720', 'X314', 'X522', 'X531', 'X625', 'bid_depth_ratio',\n",
    "        'X435', 'X293', 'X486', 'price_efficiency', 'X716', 'X627', 'X626', 'X169', 'X613',\n",
    "        'X680', 'X544', 'X115', 'X307', 'X665', 'X465', 'X347', 'X728', 'X70', 'log_volume',\n",
    "        'X340', 'X459', 'X56', 'X395', 'X354', 'X51', 'X732', 'X247', 'X324', 'X316',\n",
    "        'X76', 'X341', 'X739', 'X601', 'X386', 'X683', 'X149', 'X193', 'X628', 'X309',\n",
    "        'X351', 'X393'\n",
    "    ]\n",
    "    \n",
    "    # ADDITIONAL features requested by user (second batch)\n",
    "    additional_features_2 = [\n",
    "        'X158', 'X116', 'X74', 'X237', 'X238', 'X640', 'X251', 'X185', 'X654', 'X178',\n",
    "        'X748', 'X177', 'X26', 'X658', 'X754', 'X58', 'X521', 'X272', 'X499', 'X355',\n",
    "        'X669', 'X220', 'X223', 'X258', 'X457', 'X85', 'aggressive_ratio', 'X772', 'X537', 'X89',\n",
    "        'X389', 'X153', 'X412', 'X234', 'X478', 'X491', 'X32', 'X400', 'X334', 'X724',\n",
    "        'X210', 'normalized_spread', 'liquidity_imbalance', 'X635', 'X656', 'X236', 'X88', 'X745',\n",
    "        'X107', 'X148', 'X619', 'X304', 'X632', 'X390', 'depth_imbalance', 'X561', 'X588',\n",
    "        'X106', 'X448', 'X480', 'X65', 'X747', 'X233', 'X271', 'X138', 'X600', 'X297',\n",
    "        'X328', 'X22', 'total_liquidity', 'X350', 'X497', 'X179', 'X664', 'X174', 'X16',\n",
    "        'X67', 'X69', 'X132', 'X47', 'X365', 'X541', 'X3', 'X414', 'X408', 'X359',\n",
    "        'X121', 'X184', 'X646', 'X124', 'X552', 'X692', 'X228', 'X442', 'X736', 'X648',\n",
    "        'X11', 'X114', 'X437', 'X396', 'X54', 'X612', 'X97', 'X484', 'X222', 'X493',\n",
    "        'X542', 'X515', 'X243', 'X773', 'X135', 'X96', 'X431', 'X696', 'X353', 'liquidity_adjusted_volume',\n",
    "        'ask_depth_ratio', 'X406', 'X684', 'X454', 'X417', 'X375', 'X333', 'X503', 'X109',\n",
    "        'X691', 'X569', 'X742', 'X29', 'X418', 'X704', 'X729', 'X73', 'X226', 'X49',\n",
    "        'X94', 'X774', 'X257', 'X161', 'X475', 'X308', 'X260', 'X479', 'X207', 'X19',\n",
    "        'X337', 'X75', 'X118', 'X750', 'X9', 'X575', 'X202', 'X129', 'X232', 'X93',\n",
    "        'X229', 'X404', 'X195', 'X581', 'X549', 'X246', 'X675', 'X411', 'X327', 'X369',\n",
    "        'X765', 'X330', 'X46', 'X392', 'X504', 'X482', 'X450', 'X593', 'X181', 'X572',\n",
    "        'X274', 'X714', 'X61', 'X554', 'X657', 'X45', 'X641', 'X320', 'X319', 'X662',\n",
    "        'X744', 'X481', 'X766', 'X361', 'X78', 'X205', 'X270', 'X117', 'X487', 'X461',\n",
    "        'bid_momentum', 'X722', 'X618', 'X108', 'X764'\n",
    "    ]\n",
    "    \n",
    "    # FINAL batch of features requested by user\n",
    "    final_features = [\n",
    "        'X346', 'X216', 'bid_ask_spread', 'X139', 'X230', 'X623', 'X769', 'X630', 'X416', 'X147',\n",
    "        'X218', 'X666', 'X212', 'X336', 'X275', 'X62', 'X104', 'X146', 'X52', 'X410',\n",
    "        'X476', 'X578', 'X524', 'X273', 'X743', 'X598', 'X79', 'X225', 'X734', 'X463',\n",
    "        'X211', 'X252', 'X284', 'X133', 'X57', 'X352', 'buy_qty', 'X686', 'X456', 'X188',\n",
    "        'bid_qty', 'X48', 'X209', 'ask_momentum', 'X287', 'X201', 'X674', 'X172', 'X564', 'X77',\n",
    "        'X545', 'X620', 'X643', 'X342', 'X27', 'X366', 'X768', 'X645', 'X543', 'X595',\n",
    "        'X295', 'X763', 'X490', 'X726', 'X398', 'X264', 'execution_quality', 'X762', 'X455', 'X655',\n",
    "        'X534', 'X224', 'X156', 'X142', 'X123', 'X690', 'net_order_flow', 'X422', 'X694', 'X467',\n",
    "        'X378', 'X606', 'sell_qty', 'X423', 'X381', 'X339', 'X126', 'X708', 'X500', 'X746',\n",
    "        'X140', 'X182', 'X98', 'X164', 'X80', 'X122', 'X642', 'X276', 'X103', 'X338',\n",
    "        'X372', 'X219', 'X151', 'X231', 'X460', 'X583', 'X380', 'X50', 'X513', 'X659',\n",
    "        'X92', 'X176', 'X134', 'X269', 'X622', 'X204', 'X604'\n",
    "    ]\n",
    "    \n",
    "    # Combine all features and remove duplicates\n",
    "    all_features = list(dict.fromkeys(\n",
    "        extended_features + core_features + new_features + \n",
    "        additional_features + additional_features_2 + final_features\n",
    "    ))\n",
    "    \n",
    "    # Filter to only available columns\n",
    "    available_features = [col for col in all_features if col in df.columns]\n",
    "    \n",
    "    # Add engineered features if they exist\n",
    "    engineered_features = [\n",
    "        'log_volume_squared', 'volume_sqrt', 'order_flow_ratio', 'order_flow_diff',\n",
    "        'order_flow_imbalance_squared', 'depth_imbalance_ratio', 'total_depth', 'log_total_depth'\n",
    "    ]\n",
    "    \n",
    "    for feat in engineered_features:\n",
    "        if feat in df.columns and feat not in available_features:\n",
    "            available_features.append(feat)\n",
    "    \n",
    "    # Add momentum and volatility features\n",
    "    momentum_features = [col for col in df.columns if '_momentum' in col or '_volatility' in col]\n",
    "    for feat in momentum_features:\n",
    "        if feat not in available_features:\n",
    "            available_features.append(feat)\n",
    "    \n",
    "    print(f\"Found {len(available_features)} features out of {len(all_features)} requested\")\n",
    "    \n",
    "    return available_features\n",
    "\n",
    "class IncrementalLagEnsemble:\n",
    "    \"\"\"Incrementally trains models on different lag configurations using SGD.\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_batch_size=20, lag_batch_size=5, n_epochs=20):\n",
    "        self.feature_batch_size = feature_batch_size  # Reduced to 20 for memory efficiency\n",
    "        self.lag_batch_size = lag_batch_size\n",
    "        self.n_epochs = n_epochs  # Increased to 20 for better convergence\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.feature_names = None\n",
    "        self.model_weights = {}\n",
    "        self.performance_history = defaultdict(list)\n",
    "        \n",
    "        # Define lag strategies with odd/even splits for many configurations\n",
    "        # Reduced number of strategies to focus on most important ones for memory efficiency\n",
    "        self.lag_strategies = {\n",
    "            # Micro lags - very short term\n",
    "            'micro': [1, 2, 3, 4, 5],\n",
    "            'micro_odd': [1, 3, 5, 7, 9],\n",
    "            'micro_even': [2, 4, 6, 8, 10],\n",
    "            \n",
    "            # Ultra short with odd/even\n",
    "            'ultra_short': [6, 8, 10, 12, 15],\n",
    "            'ultra_short_odd': [7, 9, 11, 13, 15, 17],\n",
    "            'ultra_short_even': [6, 8, 10, 12, 14, 16],\n",
    "            \n",
    "            # Short with odd/even\n",
    "            'short': [20, 25, 30, 40, 50],\n",
    "            'short_odd': [21, 25, 31, 41, 51],\n",
    "            'short_even': [20, 24, 30, 40, 50],\n",
    "            \n",
    "            # Short medium with odd/even\n",
    "            'short_medium': [60, 75, 90, 105, 120],\n",
    "            'short_medium_odd': [61, 75, 91, 105, 121],\n",
    "            \n",
    "            # Medium with odd/even\n",
    "            'medium': [150, 180, 210, 240, 300],\n",
    "            'medium_odd': [151, 181, 211, 241, 301],\n",
    "            \n",
    "            # Medium long with odd/even\n",
    "            'medium_long': [360, 420, 480, 540, 600],\n",
    "            'medium_long_odd': [361, 421, 481, 541, 601],\n",
    "            \n",
    "            # Long with odd/even\n",
    "            'long': [720, 840, 960, 1080, 1200],\n",
    "            'long_odd': [721, 841, 961, 1081, 1201],\n",
    "            \n",
    "            # Very long\n",
    "            'very_long': [1440, 1800, 2160, 2520, 2880],\n",
    "            \n",
    "            # Ultra long\n",
    "            'ultra_long': [3600, 4320, 5040, 5760, 7200],\n",
    "            \n",
    "            # Special patterns\n",
    "            'hourly': [60, 120, 180, 240, 300, 360],  # Hourly patterns\n",
    "            'daily': [1440, 2880, 4320, 5760, 7200],  # Daily patterns\n",
    "        }\n",
    "        \n",
    "    def create_lag_features_batch(self, df, lag_list):\n",
    "        \"\"\"Create lag features for a batch of lags with memory optimization.\"\"\"\n",
    "        lag_features = []\n",
    "        \n",
    "        # Process lags in smaller groups to save memory\n",
    "        for i in range(0, len(lag_list), 2):\n",
    "            lag_group = lag_list[i:i+2]\n",
    "            for lag in lag_group:\n",
    "                lagged = df.shift(-lag)\n",
    "                lagged.columns = [f'{col}_lag_{lag}' for col in df.columns]\n",
    "                lag_features.append(lagged)\n",
    "            \n",
    "            # Garbage collect after each group\n",
    "            if i % 4 == 0:\n",
    "                gc.collect()\n",
    "        \n",
    "        result = pd.concat([df] + lag_features, axis=1)\n",
    "        result = result.fillna(0)\n",
    "        \n",
    "        # Clean up\n",
    "        del lag_features\n",
    "        gc.collect()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def train_sgd_model(self, X, y, model_name):\n",
    "        \"\"\"Train SGD model incrementally with adaptive learning and regularization.\"\"\"\n",
    "        print(f\"  Training SGD model: {model_name} ({self.n_epochs} epochs)\")\n",
    "        \n",
    "        # Initialize model and scaler if not exists\n",
    "        if model_name not in self.models:\n",
    "            # Enhanced SGD configuration for better convergence\n",
    "            self.models[model_name] = SGDRegressor(\n",
    "                loss='huber',\n",
    "                penalty='elasticnet',\n",
    "                alpha=0.00005,  # Reduced regularization\n",
    "                l1_ratio=0.15,\n",
    "                learning_rate='adaptive',  # Changed to adaptive\n",
    "                eta0=0.015,  # Slightly higher initial learning rate\n",
    "                power_t=0.3,  # Adjusted decay\n",
    "                random_state=42,\n",
    "                warm_start=True,\n",
    "                max_iter=1500,  # Increased iterations\n",
    "                tol=5e-4,  # Slightly looser tolerance\n",
    "                epsilon=0.1,  # Huber loss threshold\n",
    "                average=True  # Use averaged SGD\n",
    "            )\n",
    "            self.scalers[model_name] = StandardScaler()\n",
    "            \n",
    "        model = self.models[model_name]\n",
    "        scaler = self.scalers[model_name]\n",
    "        \n",
    "        # Train in epochs with smaller chunks for better convergence\n",
    "        chunk_size = 10000  # Reduced to 10000 for memory efficiency\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # Shuffle indices for each epoch\n",
    "            indices = np.random.permutation(len(X))\n",
    "            \n",
    "            # Learning rate decay schedule\n",
    "            if epoch > 0 and epoch % 5 == 0:\n",
    "                model.set_params(eta0=model.eta0 * 0.9)\n",
    "            \n",
    "            for start_idx in range(0, len(X), chunk_size):\n",
    "                end_idx = min(start_idx + chunk_size, len(X))\n",
    "                \n",
    "                # Get shuffled chunk\n",
    "                chunk_indices = indices[start_idx:end_idx]\n",
    "                X_chunk = X.iloc[chunk_indices]\n",
    "                y_chunk = y[chunk_indices]\n",
    "                \n",
    "                # Scale\n",
    "                if start_idx == 0 and epoch == 0:\n",
    "                    X_scaled = scaler.fit_transform(X_chunk)\n",
    "                else:\n",
    "                    X_scaled = scaler.transform(X_chunk)\n",
    "                \n",
    "                # Partial fit\n",
    "                model.partial_fit(X_scaled, y_chunk)\n",
    "                \n",
    "                # Aggressive garbage collection\n",
    "                if start_idx % (chunk_size * 2) == 0:\n",
    "                    gc.collect()\n",
    "            \n",
    "            # Print progress\n",
    "            if epoch % 4 == 0:\n",
    "                print(f\"    Epoch {epoch+1}/{self.n_epochs} completed\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_feature_batch(self, feature_batch, X_full, y, strategy_name, lag_list):\n",
    "        \"\"\"Train on a batch of features with specific lags.\"\"\"\n",
    "        print(f\"\\n  Processing feature batch ({len(feature_batch)} features) with {strategy_name} lags\")\n",
    "        \n",
    "        # Select feature batch\n",
    "        X_batch = X_full[feature_batch].copy()\n",
    "        \n",
    "        # Create lag features\n",
    "        X_with_lags = self.create_lag_features_batch(X_batch, lag_list)\n",
    "        \n",
    "        # Train SGD model\n",
    "        model_name = f\"{strategy_name}_{feature_batch[0]}_{feature_batch[-1]}\"\n",
    "        self.train_sgd_model(X_with_lags, y, model_name)\n",
    "        \n",
    "        # Clean up\n",
    "        del X_batch, X_with_lags\n",
    "        gc.collect()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit ensemble using incremental training.\"\"\"\n",
    "        print(\"Training Enhanced Incremental Lag Ensemble...\")\n",
    "        print(f\"Total epochs per model: {self.n_epochs}\")\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        n_features = len(self.feature_names)\n",
    "        \n",
    "        # Split features into batches\n",
    "        feature_batches = []\n",
    "        for i in range(0, n_features, self.feature_batch_size):\n",
    "            batch = self.feature_names[i:i+self.feature_batch_size]\n",
    "            feature_batches.append(batch)\n",
    "        \n",
    "        print(f\"Split {n_features} features into {len(feature_batches)} batches\")\n",
    "        print(f\"Total lag strategies: {len(self.lag_strategies)}\")\n",
    "        \n",
    "        # Calculate total lag values\n",
    "        all_lags = set()\n",
    "        for lags in self.lag_strategies.values():\n",
    "            all_lags.update(lags)\n",
    "        print(f\"Total unique lag values: {len(all_lags)}\")\n",
    "        \n",
    "        # Train models for each combination of feature batch and lag strategy\n",
    "        total_models = len(feature_batches) * len(self.lag_strategies)\n",
    "        model_count = 0\n",
    "        \n",
    "        for strategy_name, lag_list in self.lag_strategies.items():\n",
    "            print(f\"\\nTraining {strategy_name} strategy (lags: {lag_list})\")\n",
    "            \n",
    "            for batch_idx, feature_batch in enumerate(feature_batches):\n",
    "                model_count += 1\n",
    "                print(f\"Progress: {model_count}/{total_models} models\")\n",
    "                \n",
    "                self.train_feature_batch(feature_batch, X, y, strategy_name, lag_list)\n",
    "                \n",
    "                # Aggressive cleanup after every model\n",
    "                gc.collect()\n",
    "        \n",
    "        # Initialize equal weights\n",
    "        for model_name in self.models:\n",
    "            self.model_weights[model_name] = 1.0 / len(self.models)\n",
    "        \n",
    "        print(f\"\\nTotal models trained: {len(self.models)}\")\n",
    "        \n",
    "    def predict_batch(self, X, feature_batch, strategy_name, lag_list):\n",
    "        \"\"\"Make predictions for a specific feature batch and lag strategy.\"\"\"\n",
    "        model_name = f\"{strategy_name}_{feature_batch[0]}_{feature_batch[-1]}\"\n",
    "        \n",
    "        if model_name not in self.models:\n",
    "            return None\n",
    "            \n",
    "        # Select features\n",
    "        X_batch = X[feature_batch].copy()\n",
    "        \n",
    "        # Create lag features\n",
    "        X_with_lags = self.create_lag_features_batch(X_batch, lag_list)\n",
    "        \n",
    "        # Scale and predict\n",
    "        X_scaled = self.scalers[model_name].transform(X_with_lags)\n",
    "        predictions = self.models[model_name].predict(X_scaled)\n",
    "        \n",
    "        # Clean up\n",
    "        del X_batch, X_with_lags, X_scaled\n",
    "        gc.collect()\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make ensemble predictions with weighted averaging.\"\"\"\n",
    "        all_predictions = []\n",
    "        weights = []\n",
    "        \n",
    "        # Recreate feature batches\n",
    "        n_features = len(self.feature_names)\n",
    "        feature_batches = []\n",
    "        for i in range(0, n_features, self.feature_batch_size):\n",
    "            batch = self.feature_names[i:i+self.feature_batch_size]\n",
    "            if all(col in X.columns for col in batch):\n",
    "                feature_batches.append(batch)\n",
    "        \n",
    "        # Get predictions from each model\n",
    "        prediction_count = 0\n",
    "        for strategy_name, lag_list in self.lag_strategies.items():\n",
    "            for feature_batch in feature_batches:\n",
    "                pred = self.predict_batch(X, feature_batch, strategy_name, lag_list)\n",
    "                if pred is not None:\n",
    "                    all_predictions.append(pred)\n",
    "                    model_name = f\"{strategy_name}_{feature_batch[0]}_{feature_batch[-1]}\"\n",
    "                    weights.append(self.model_weights.get(model_name, 1.0))\n",
    "                    prediction_count += 1\n",
    "                    \n",
    "                    # Garbage collect after every 5 predictions\n",
    "                    if prediction_count % 5 == 0:\n",
    "                        gc.collect()\n",
    "        \n",
    "        # Weighted average\n",
    "        if all_predictions:\n",
    "            weights = np.array(weights) / np.sum(weights)\n",
    "            return np.average(all_predictions, axis=0, weights=weights)\n",
    "        else:\n",
    "            return np.zeros(len(X))\n",
    "\n",
    "# Main execution starts here\n",
    "print(\"=\"*70)\n",
    "print(\"ULTRA-ENHANCED CRYPTO PREDICTION WITH COMPLETE FEATURE SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Set pandas options\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Optimize memory\n",
    "train_df = optimize_memory(train_df, verbose=True)\n",
    "\n",
    "# Create advanced features\n",
    "train_df = create_advanced_features(train_df)\n",
    "\n",
    "# Extract labels\n",
    "y_train = train_df['label'].to_numpy().astype(np.float32)\n",
    "\n",
    "# Select features\n",
    "feature_cols = get_feature_columns(train_df)\n",
    "X_train = train_df[feature_cols].copy()\n",
    "\n",
    "# Clean up\n",
    "del train_df\n",
    "gc.collect()\n",
    "\n",
    "# Further optimize X_train\n",
    "X_train = optimize_memory(X_train, verbose=True)\n",
    "\n",
    "# Create and train the ensemble with more epochs\n",
    "ensemble = IncrementalLagEnsemble(\n",
    "    feature_batch_size=20,  # Reduced to 20 for memory efficiency\n",
    "    lag_batch_size=5,\n",
    "    n_epochs=20  # Increased to 20 for better convergence\n",
    ")\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Clean up training data\n",
    "del X_train, y_train\n",
    "gc.collect()\n",
    "\n",
    "# Load test data\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LOADING TEST DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "test_df = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/test.parquet')\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "# Optimize memory\n",
    "test_df = optimize_memory(test_df, verbose=True)\n",
    "\n",
    "# Create advanced features for test data\n",
    "test_df = create_advanced_features(test_df)\n",
    "\n",
    "# Timestamp reconstruction\n",
    "timestamp_recon_path = '/kaggle/input/the-order-of-the-test-rows-2/closest_rows.csv'\n",
    "use_timestamp_reconstruction = os.path.exists(timestamp_recon_path)\n",
    "\n",
    "if use_timestamp_reconstruction:\n",
    "    print(\"\\nApplying timestamp reconstruction...\")\n",
    "    \n",
    "    t = pd.Series(pd.read_csv(timestamp_recon_path)['0'].to_numpy())\n",
    "    print(f\"Timestamps loaded: {len(t)}\")\n",
    "    \n",
    "    # Process timestamps\n",
    "    t -= 10080\n",
    "    t[t < 0] = 538149\n",
    "    \n",
    "    t = t.sort_values()\n",
    "    t[t <= len(t)] = np.arange(t[t <= len(t)].shape[0])\n",
    "    t = t.sort_index()\n",
    "    \n",
    "    t = pd.Series(np.arange(538150), index=t.to_numpy()).sort_index()\n",
    "    \n",
    "    # Sort test data\n",
    "    test_df = test_df.iloc[t.to_numpy()]\n",
    "    print(\"Test data sorted by reconstructed timestamps\")\n",
    "else:\n",
    "    print(\"No timestamp reconstruction file found\")\n",
    "    t = pd.Series(np.arange(len(test_df)))\n",
    "\n",
    "# Select same features as training\n",
    "X_test = test_df[feature_cols].copy()\n",
    "del test_df\n",
    "gc.collect()\n",
    "\n",
    "# Optimize test features\n",
    "X_test = optimize_memory(X_test, verbose=True)\n",
    "\n",
    "# Make predictions in chunks\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MAKING PREDICTIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "chunk_size = 5000  # Reduced to 5000 for extreme memory efficiency\n",
    "n_samples = len(X_test)\n",
    "y_pred = np.zeros(n_samples, dtype=np.float32)\n",
    "\n",
    "n_chunks = (n_samples + chunk_size - 1) // chunk_size\n",
    "print(f\"Processing {n_chunks} chunks of size {chunk_size}\")\n",
    "\n",
    "for i in range(0, n_samples, chunk_size):\n",
    "    end_idx = min(i + chunk_size, n_samples)\n",
    "    chunk_num = i // chunk_size + 1\n",
    "    \n",
    "    if chunk_num % 20 == 0:\n",
    "        print(f\"\\nChunk {chunk_num}/{n_chunks} (rows {i}-{end_idx})\")\n",
    "    \n",
    "    # Get chunk\n",
    "    X_chunk = X_test.iloc[i:end_idx]\n",
    "    \n",
    "    # Predict\n",
    "    y_pred[i:end_idx] = ensemble.predict(X_chunk).astype(np.float32)\n",
    "    \n",
    "    # Aggressive cleanup after every chunk\n",
    "    gc.collect()\n",
    "\n",
    "# Clean up test data\n",
    "del X_test\n",
    "gc.collect()\n",
    "\n",
    "# Display statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREDICTION STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "pred_series = pd.Series(y_pred)\n",
    "print(pred_series.describe())\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Cumulative sum\n",
    "axes[0, 0].plot(np.cumsum(y_pred))\n",
    "axes[0, 0].set_title('Cumulative Predictions')\n",
    "axes[0, 0].set_xlabel('Index')\n",
    "axes[0, 0].set_ylabel('Cumulative Sum')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution\n",
    "axes[0, 1].hist(y_pred, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_title('Prediction Distribution')\n",
    "axes[0, 1].set_xlabel('Value')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "\n",
    "# First 2000 predictions\n",
    "axes[1, 0].plot(y_pred[:2000], alpha=0.7)\n",
    "axes[1, 0].set_title('First 2000 Predictions')\n",
    "axes[1, 0].set_xlabel('Index')\n",
    "axes[1, 0].set_ylabel('Prediction')\n",
    "\n",
    "# Rolling mean and std\n",
    "window = 1000\n",
    "rolling_mean = pred_series.rolling(window).mean()\n",
    "rolling_std = pred_series.rolling(window).std()\n",
    "\n",
    "axes[1, 1].plot(rolling_mean, label='Mean')\n",
    "axes[1, 1].fill_between(\n",
    "    range(len(rolling_mean)),\n",
    "    rolling_mean - rolling_std,\n",
    "    rolling_mean + rolling_std,\n",
    "    alpha=0.3,\n",
    "    label='±1 Std'\n",
    ")\n",
    "axes[1, 1].set_title(f'Rolling Statistics (window={window})')\n",
    "axes[1, 1].set_xlabel('Index')\n",
    "axes[1, 1].set_ylabel('Value')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional analysis plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "# Prediction volatility over time\n",
    "window_sizes = [100, 500, 1000, 5000]\n",
    "for window in window_sizes:\n",
    "    rolling_vol = pred_series.rolling(window).std()\n",
    "    ax.plot(rolling_vol, label=f'Window {window}', alpha=0.7)\n",
    "\n",
    "ax.set_title('Prediction Volatility Over Time')\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Rolling Standard Deviation')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Prepare submission\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREPARING SUBMISSION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "submission = pd.read_csv('/kaggle/input/drw-crypto-market-prediction/sample_submission.csv')\n",
    "\n",
    "if use_timestamp_reconstruction:\n",
    "    submission = submission.iloc[t.to_numpy()]\n",
    "    submission['prediction'] = y_pred\n",
    "    submission = submission.sort_index()\n",
    "else:\n",
    "    submission['prediction'] = y_pred\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission saved to 'submission.csv'\")\n",
    "\n",
    "# Display submission info\n",
    "print(\"\\nSubmission preview:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "print(f\"Prediction range: [{submission['prediction'].min():.6f}, {submission['prediction'].max():.6f}]\")\n",
    "print(f\"Mean: {submission['prediction'].mean():.6f}\")\n",
    "print(f\"Std: {submission['prediction'].std():.6f}\")\n",
    "\n",
    "# Percentile information\n",
    "percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]\n",
    "print(\"\\nPrediction percentiles:\")\n",
    "for p in percentiles:\n",
    "    value = np.percentile(submission['prediction'], p)\n",
    "    print(f\"  {p}th percentile: {value:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PROCESS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nModel Summary:\")\n",
    "print(f\"- Total features used: {len(feature_cols)}\")\n",
    "print(f\"- Feature batches: {len(feature_cols) // ensemble.feature_batch_size + 1}\")\n",
    "print(f\"- Lag strategies: {len(ensemble.lag_strategies)}\")\n",
    "print(f\"- Total models: {len(ensemble.models)}\")\n",
    "print(f\"- Model type: Enhanced SGDRegressor with adaptive learning\")\n",
    "print(f\"- Training: Incremental with partial_fit ({ensemble.n_epochs} epochs)\")\n",
    "print(f\"- Prediction: Weighted ensemble average\")\n",
    "\n",
    "# Count total unique lag values\n",
    "all_lags = set()\n",
    "for lags in ensemble.lag_strategies.values():\n",
    "    all_lags.update(lags)\n",
    "print(f\"- Total unique lag values: {len(all_lags)}\")\n",
    "\n",
    "print(\"\\nEnhanced Features:\")\n",
    "print(\"- Advanced feature engineering applied\")\n",
    "print(\"- Complete feature set (700+ features)\")\n",
    "print(\"- Improved SGD configuration with adaptive learning\")\n",
    "print(\"- Enhanced lag strategies including special patterns\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
